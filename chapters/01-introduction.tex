\chapter{Introduction and physics background.}
\section{Units and conventions.}
\label{sec:units_and_conventions}
    Even though nature does not establish preferred units, the physics that describes it requires establishing a common language and framework.
    %
    In high energy physics, the choice of units reflects a not simply an arbitrary choice, but a philosophical stance about the nature of reality.
    %
    While everyday experience suggests that meters, kilograms, and seconds might be the most suitable units to express the physical world, particle physics reveals that speed, action, and energy form more natural rulers for measuring the universe at its smallest scales.
    %
    \begin{definition}
        \emph{Natural Units} are a system where \(\hbar = c = 1\), effectively setting the speed of light and quantum of action as fundamental measuring sticks.
        %
        This choice transforms all physical quantities into powers of energy.
    \end{definition}
    
\input{tables/1-1.tex}
    Some common physical quantities, their abbreviated natural units, their full natural units, their approximate SI equivalents and their physical significance are described in \cref{tab:unit-conversions-combined}.
    
\subsection{Coordinate systems and relativistic geometry.}
    At a high energy physics (HEP) experiment, the choice of coordinates must respect the underlying symmetries of the physics and the experimental setup.
    %
    The laboratory frame typically provides a natural Cartesian system.
    %
    For example, for a circular collider experiment, one can set the  z--axis to be along the beam direction (longitudinal), the x--axis to be horizontal, pointing toward the centre of the accelerator ring and the y--axis to be vertical, completing the right handed system.

    However, the partons that participate in an interaction can carry unknown fractions of the beam momentum, making the centre of mass frame of each interaction unknowable.
    %
    This uncertainty motivates a coordinate system that transforms simply under longitudinal Lorentz boosts.
    \begin{definition}
        The \emph{pseudorapidity} \(\eta,\) is defined as
        \[
            \eta = -\ln \tan\frac\theta2
            \]
        where \(\theta\) is the polar angle from the beam axis~\cite{rubbia_phenomenology_2022}.
        %
        Under a longitudinal boost with rapidity \(\beta = \operatorname{artanh}\nicefrac{v}{c}\), a massless particle's \(\eta\) transforms as \(\eta\mapsto\eta+\beta\)
    \end{definition}
    Expressing \(\theta\) in terms of momentum,
    \[
        \eta = -\ln \tan \frac\theta2 = \frac12\ln\frac{|p| + p_z}{|p| - p_z} = \operatorname{artanh}\frac{p_z}{|p|}
    \]
    \cref{fig:eta_vs_y} shows the relationship between pseudorapidity $\eta$ and true rapidity $y$ for particles with different mass to $p_T$ ratios.
    %
    As expected, the massless case ($m/p_T=0$) lies exactly on the diagonal $y=\eta$, while non-zero $\nicefrac{m}{p_T}$ introduces a systematic deviation that grows at large $|\eta|$.
    %
    Even a modest ratio ($m/p_T=0.1$) produces a measurable shift, and by $m/p_T=2.0$ the rapidity is significantly reduced relative to $\eta$.
    %
    This behaviour must be accounted for when inferring kinematic distributions of heavy particles from detector measurements expressed in pseudorapidity.
    
\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figures/chapter-01/rapidity.pdf}
  \caption[\(\eta\) vs. \(y\) for various $\nicefrac{m}{p_T}$]{%
    Comparison of pseudorapidity $\eta$ and true rapidity
    $y=\frac12\ln\frac{E+p_z}{E-p_z}$ for several mass to transverse momentum ratios.
    %
    Curves correspond to $\nicefrac{m}{p_T} = 0.0,\;0.1,\;0.5,\;1.0,\;2.0$, illustrating how finite mass distorts away from the massless limit $y=\eta$ at large $|\eta|$.
  }
  \label{fig:eta_vs_y}
\end{figure}
For massless particles, differences in \(\eta\) remain invariant under longitudinal boosts, motivating the definition of the angular distance as follows.

\begin{definition}
    The \emph{angular distance metric} is defined as
    \[
        \Delta R^2 = \Delta\eta^2 + \Delta\varphi^2,
    \]
    where \(\varphi\) is the azimuthal angle around the beam axis.
    %
    This metric approximates the geometric angle between particles in the detector while remaining approximately boost invariant.
\end{definition}
    \subsection{Statistical frameworks and uncertainty quantification.}
        In modern particle physics analyses, every measurement emerges from millions of interaction events, each carrying both statistical and systematic uncertainties.
        %
        \begin{definition}
            The \emph{Poisson distribution} is the fundamental distribution governing counting experiments.
            %
            For \(n\) observed events with expected value \(\lambda\),
            \[
                \P(n\mid\lambda) = \frac{\lambda^n e^{-\lambda}}{n!}
            \]
        \end{definition}

    In the high statistics limit, (\(n \to\infty\)), the Poisson distribution approaches a Gaussian.
    
    \begin{definition}
        The \emph{profile likelihood ratio}
            \[
                \mathcal L(\mu) = \frac{L(\mu, \hat\theta_\mu)}{ L(\hat\mu, \hat\theta)}
            \]
            where \(\hat\theta_\mu\) maximises \(L\) for fixed signal strength \(\mu\) is the optimal test statistics for hypothesis testing under the Neyman--Pearson lemma.

    \subsection{Machine learning architectures and notation.}
        Neural networks can be specified by their architecture vectors, \( [d_0, d_1,\dots, d_\ell]\), which denotes a network with input dimension \(d_0\), hidden layers of dimensions \(d_1\) through \(d_{\ell-1},\) and output dimension \(d_\ell\).

        For a network \(f: \R^{d_0} \to\R^{d_{\ell}}\) with parameters \(\theta = \qty{W_i, b_i}\), the forward pass computes
        \begin{gather}
            h_0 = x\\
            h_i = \sigma(W_i\,h_{i-i} + b_i)\quad i = 1,\dots, \ell-1\\
            f(x;\theta) = W_\ell\,h_{\ell - 1} + b_\ell
        \end{gather}
        where \(\sigma\) denotes the activation function. The universality theorem guarantees that sufficiently wide networks can approximate any continuous function.

        Training proceeds via gradient descent on a loss function \(L(\theta)\), with the gradient computed through automatic differentiation.
        
    \subsection{Information theory and optimal observables.}
        Information theory provides a rigorous framework through the Neyman-Pearson lemma, which implies that the likelihood ratio \(\frac{L(x\mid S)}{L(x\mid B)}\) provides the most powerful test for distinguishing signal \(S\) from background \(B\) at any given significance level.

        In practice, this optimal observable can be approximated using machine learning classifiers.
        %
        A well trained classifier computes
        \[
            f(x) \approx \P(S|x) = \frac{L(x\mid S)\,\P(S)}{L(x\mid S)\,\P(S) + L(x\mid B)\, \P(B)]}
        \]
        The mutual information \(I(Y; f(X))\) between the true labels \(Y\) and classifier output \(f(X)\) quantifies the information captured
        \[
            I(Y; f(X)) = \iint p(y, f)\,\log \frac{p(y, f)}{p(y)\,p(f)} \dd y\, \dd f.
        \]
        This connects directly to the area under the ROC curve and provides a model-independent measure of classification performance.

\subsection{Datasets.}
    Meaningful distinctions in particle physics data arise from the transformation between the underlying physical processes one seeks to understand and the experimental observations one can actually record, as well as from the distinction between Monte Carlo simulation and measurement in nature.
    %
    Datasets can therefore be classified according to two principal axes that capture the essential distinctions in particle physics data.
    
    The first axis distinguishes between events that occur in nature versus those generated through Monte Carlo simulation.
    %
    The second axis separates particle level quantities interest from detector level ones.

    This classification yields four distinct dataset categories, each serving a unique role in the measurement process.
    %
    Understanding the relationships between these categories plays an important role in the development of the unfolding methods discussed in this dissertation.
    \begin{figure}
    \centering
    \resizebox{\linewidth}{!}{\input{figures/chapter-01/dataset-taxonomy.tex}}
    \caption[Taxonomy of datasets used]{The four categories of data used in this dissertation organised along two orthogonal dimensions.
    %
    The vertical axis distinguishes between natural phenomena (Nature) and computational models (MC), while the horizontal axis separates particle level quantities from detector level observations.
    %
    Arrows indicate the detector response that maps particle level events to their detector level manifestations.
    }
    \label{fig:dataset-taxonomy}
    \end{figure}

\cref{fig:dataset-taxonomy} illustrates this taxonomy graphically.

    \subsubsection{Nature versus Monte Carlo.}
        The vertical dimension of \cref{fig:dataset-taxonomy} distinguishes between natural phenomena and their simulated counterparts.
        %
        Natural events represent the actual physical processes occurring in particle interactions, while Monte Carlo events are computational reproductions based on theoretical understanding.
        %
        This distinction is fundamental because although natural events contain true physics, Monte Carlo simulations provide the controlled environment necessary to understand the detector response and develop analysis techniques.

        Monte Carlo simulations incorporate theoretical knowledge, including matrix element calculations, parton shower models, and hadronisation prescriptions.
        %
        While these simulations can achieve remarkable fidelity to natural processes, they remain approximations subject to theoretical uncertainties and modelling assumptions.

    \subsubsection{Particle level versus detector level.}
        The horizontal dimension captures the transformation from particle level information to detector level signals.
        %
        Particle level quantities represent the properties of particles immediately after the hard scattering process and subsequent decay chains, before interaction with detector.
        %
        These quantities form the natural language of theoretical predictions and are directly calculable from field theory.

        Detector level quantities, in contrast, represent the actual experimental observables after particles have traversed the detector apparatus.
        %
        This transformation incorporates numerous physical effects.
        %
        The detector acts as a complex transfer function that irreversibly transforms the particle level distributions into the measured quantities.

    \subsubsection{Four dataset categories.}
        The intersection of these two dimensions yields four fundamental dataset categories.
        \begin{description}
            \item[Truth]  represents nature at the particle level---the actual physical distributions HEP experiments ultimately seek to measure.
            %
            These distributions are never directly observable but represent the ground truth that all analyses attempt to approach.
            %
            In practice, truth distributions are estimated through unfolding procedures from the information contained in the other three distributions.
            \item [Data] comprises natural events after detector effects.
            %
            It is the raw experimental output of all measurements.
            %
            Data embodies the convolution of the physics of interest with instrumental effects, making it the starting point for any experimental analysis.
            %
            The challenge lies in disentangling the physics content from the detector effects.
            \item [Generation] comprises Monte Carlo events at the particle level, providing a controlled means to understanding theoretical predictions.
            %
            Generation allows the study of particle level distributions with perfect knowledge and unlimited statistics, subject to theoretical modelling uncertainties.
            %
            Generated events serve as proxies for truth in developing and validating analysis methods.
            \item [Simulation] represents Monte Carlo events processed through detailed detector modelling, creating artificial detector level datasets with known particle level origins.
            %
            Simulation enables the study the detector response function by providing matched pairs of particle level and detector level quantities.
            %
            The fidelity of detector simulation directly impacts the ability to correct for instrumental effects.
        \end{description}
    \subsubsection{The detector response.}
        The arrows in \cref{fig:dataset-taxonomy} represent the action of the detector response function, mapping particle level distributions to their detector level counterparts.
        %
        This mapping is inherently stochastic, incorporating both resolution effects that blur distributions and acceptance effects that create blind regions in phase space.

        For natural events, this transfer function represents the actual physical processes of particle interaction with detector material---ionisation in tracking chambers, showering in calorimeters, and signal processing in readout electronics.
        %
        For simulated events, the detector simulation models these processes through programs like \textsc{Geant4} that track particles through detailed detector geometries, applying the current best understanding of particle matter interactions.

    \subsubsection{Operational framework.}
        In practice, these four dataset categories work in concert to enable precision measurements.
        %
        Paired Generation--Simulation samples provide the response matrix linking particle and detector levels, while control regions in Data validate the simulation fidelity.
        %
        Closure tests using independent Monte Carlo samples verify the unfolding procedure, and the final measurement applies these validated corrections to data to infer the underlying Truth.

        This framework implicitly assumes that the detector response is sufficiently similar between data and simulation, an assumption that must be continuously validated through auxiliary measurements and control samples.
        %
        The robustness of any measurement depends on understanding the limitations of this assumption and properly propagating the associated systematic uncertainties.

        The subsequent chapters of this dissertation will repeatedly reference these four dataset categories in the development of sophisticated methods for extracting particle level information from detector level observations.
        %
        This terminology provides the consistent language necessary for discussing the various approaches to the unfolding problem and their relative merits in different physics contexts.      
    \subsection{A note on conventions and clarity.}
         This work strives to maintain a balance between mathematical rigour and physical insight.
         %
         Where conventions differ between communities,\footnote{For example, particle physicists and machine learning researchers.} attempts are made to note both conventions.
         
\section{The Standard Model: Theoretical framework.}
The Standard Model of particle physics represents one of the most significant intellectual achievements in modern science.
%
Developed throughout the latter half of the $20^{\text{th}}$ century, it provides a quantum field theory framework that describes three of the four known fundamental forces—the electromagnetic, weak, and strong interactions---in addition to classifying all known elementary particles.
%
The mathematical formulation of the Standard Model is based on gauge theory, specifically quantum chromodynamics (QCD) and the electroweak theory, underpinned by the gauge symmetry group \(SU(3)_C\times SU(2)_L\times U(1)_Y\).

The predictive power of the Standard Model has been repeatedly validated through precision experiments across multiple energy scales, from low energy nuclear phenomena to the highest energy particle collisions achievable at modern accelerators.
%
Its crowning achievement came with the discovery of the Higgs boson in 2012 at the Large Hadron Collider (LHC)~\cite{ATLAS:2012yve, collaboration_observation_2012}, confirming the mechanism through which elementary particles acquire mass.
%
A schematic illustration of the standard model can be found in \cref{fig:sm}.

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{figures/chapter-01/Standard_Model_of_Elementary_Particles.svg.png}
    \caption{
        A schematic illustration of the Standard Model~\cite{wiki:stdmodel}.
    }
    \label{fig:sm}
\end{figure}
\subsection{Fundamental particles and forces.}
The Standard Model categorises elementary particles into two main families: fermions, which comprise matter, and bosons, which mediate forces between matter particles.
\subsubsection{Fermions: The building blocks of matter.}
Fermions, characterized by half--integer spin, obey the Pauli exclusion principle and satisfy Fermi--Dirac statistics.
%
Fermions are further classified into quarks and leptons, each arranged in three generations of increasing mass.
\paragraph{Quarks.}
Quarks are spin $ = \nicefrac{1}{2}$ particles that are catagorised into three generations as follows:
\begin{itemize}
    \item Up (u) and down (d),
    \item Charm (c) and strange (s),
    \item Top (t) and bottom (b).
\end{itemize}

Quarks carry fractional electric charge and colour charge, and experience all fundamental forces.
%
They are confined within hadrons---composite particles categorized as baryons\footnote{three quark states, like protons and neutrons.} or mesons\footnote{quark--antiquark pairs.}.

\paragraph{Leptons.}
Like quarks, leptons too are spin \(\nicefrac12\) particles that are catagorised into three generations.
\begin{itemize}
    \item Electron (e) and electron neutrino ($\nu_e$),
    \item Muon $(\mu)$ and muon neutrino ($\nu_\mu$),
    \item Tau ($\tau$) and tau neutrino ($\nu_\tau$).
\end{itemize}

Electrons, muons, and taus carry unit electric charge and interact through the electromagnetic and weak forces, while neutrinos are electrically neutral and interact only through the weak force, making them notoriously difficult to detect.

\subsubsection{Bosons: Force carriers.}
Bosons, with integer spin values, mediate the fundamental interactions.
%
They are not subject to the Pauli exclusion principle and instead satisfy Bose--Einstein statistics.
%
The Standard Model comprises the following bosons:
\begin{description}
    \item [The photon $(\gamma)$] is a massless spin--1 boson that mediates the electromagnetic force.
    \item [$W^\pm$ and $Z$ bosons] are massive spin--1 bosons that mediate the weak force.
    \item [Gluons $(g)$] are a set of eight massless spin--1 bosons that mediate the strong force.
    \item [The Higgs boson ($H$)] is a massive spin--0 boson associated with the Higgs field that gives mass to elementary particles.
\end{description}

\subsection{Theoretical framework and symmetries.}
The Standard Model is constructed through principles of quantum field theory where particles are excitations of underlying quantum fields.
%
Its mathematical structure is determined by local gauge invariance under the following specific symmetry transformations:
\begin{description}
    \item [$\vb*{U(1)_Y}$] is associated with electroweak hypercharge and is the symmetry of electroweak theory,
    \item [$\vb*{SU(2)_L}$] describes the weak isospin, and acts on left-handed fermions, and
    \item [$\vb*{SU(3)_C}$] governs the strong interactions through colour charge in QCD.
\end{description}

Electroweak unification, demonstrated by Glashow~\cite{Glashow:1961tr}, Weinberg~\cite{Weinberg:1979pi}, and Salam~\cite{Salam:1980jd}, demonstrates how the electromagnetic and weak forces emerge as different aspects of a single electroweak interaction, which undergoes spontaneous symmetry breaking at low energies.

\subsection{The Higgs mechanism and mass generation.}
The Higgs mechanism, proposed by Peter Higgs in the 1960s~\cite{PhysRevLett.13.508}, addresses the theoretical inconsistency of massive gauge bosons in a gauge invariant theory.
%
The mechanism introduces a scalar field---the Higgs field---that permeates space and spontaneously broke the electroweak symmetry when the universe cooled after the Big Bang.

This symmetry breaking generates masses for the W and Z bosons while leaving the photon massless, explaining the significant difference between the electromagnetic and weak forces at ordinary energies. Additionally, the Higgs field couples to fermions through Yukawa interactions~\cite{DHoker1984DecouplingTheory}, generating their masses with coupling strengths proportional to the particle masses.

The discovery of the Higgs boson at the LHC in 2012, with properties consistent with Standard Model predictions, provided crucial experimental validation of this mechanism and completed the Standard Model's particle roster.

\subsection{Limitations and beyond the Standard Model (BSM) physics.}
Despite its remarkable success, the Standard Model has several well recognised limitations, including,

    \begin{enumerate}
        \item It does not incorporate gravity, the fourth fundamental force.
        \item It fails to explain the observed matter--antimatter asymmetry in the universe.
        \item It does not account for dark matter or dark energy, which together constitute about 95\% of the universe's energy content.
        \item  It requires fine tuning of parameters, raising theoretical concerns like the hierarchy problem.
        \item  It does not explain neutrino masses, which must exist given observed neutrino oscillations.
    \end{enumerate}
These limitations motivate theoretical extensions and experimental searches for physics beyond the Standard Model, including supersymmetry, grand unified theories, and various dark matter candidates.
%
Precision measurements at particle physics experiments provide one of the most powerful approaches to probe these potential extensions, making analysis techniques like those discussed in this thesis essential for advancing our fundamental understanding of nature.

\section{Fundamental role of cross section measurements in particle physics.}

Differential cross section measurements are the fundamental currency of scientific exchange in particle physics, serving as the primary bridge between theoretical predictions and experimental observations.
%
These measurements quantify the probability density of specific particle interactions as a function of kinematic variables, providing the essential link between theoretical predictions and experimental observations.
%
 A cross section quantifies the probability of a specific particle interaction occurring and is typically expressed in units of area (barns).\footnote{1 barn = \siqty{e-24}{\square\cm}.} This seemingly simple concept forms the cornerstone of how we test and validate our understanding of fundamental physics.
 
The Standard Model makes precise predictions for cross sections that can be directly tested at particle physics experiments.
%
Any statistically significant deviation between measured cross sections and theoretical predictions may signal the presence of new physics beyond the Standard Model~\cite{particle_data_group_review_2022}.\footnote{Such deviations might also signal errors in the theoretical framework used for predictions or in the experimental procedures used to measure the cross section.
} 

Cross sections are particularly powerful because they encode the underlying quantum field theory structure in a form that can be directly probed by experiment.
%
For instance, measurements of jet production cross sections at different energy scales reveal the running of the strong coupling constant \(\alpha_S\)~\cite{chiefa_parton_2025}, while precision electroweak cross section measurements constrain the properties of the Higgs boson and other fundamental particles~\cite{noauthor_precision_2006}.
%
In searches for physics beyond the Standard Model, differential cross section measurements can reveal subtle deviations that point to new particles or interactions, even when direct observation is beyond experimental reach.

These measurements also serve a crucial role in constraining effective field theories (EFTs) that parameterise potential new physics in a model independent way.
%
By measuring differential distributions with high precision, experiments can place bounds on EFT coefficients, narrowing the space of viable theoretical extensions to the Standard Model~\cite{contino_validity_2016}.

For example, the ongoing precision program at the Large Hadron Collider (LHC) relies heavily on refined cross section measurements to extract maximum physical insight from collected data.
%
In addition to driving comparisons with theoretical models, cross section measurements are also used at high energy physics experiments for MC tuning~\cite{albert_antares_2025} and consistency checks~\cite{buckley_constraints_2025} among other applications.

\section{Cross section measurements: From theory to experiment.}
\subsection{Theory.}
    Classically, the cross section (\(\sigma\)) represents the effective area within which two particles must interact for a particular process to occur.
    %
    \begin{definition}
        For collisions between discrete particles, the \emph{cross section} is defined as the area transverse to their relative motion.
        %
        If the particles were to interact via contact forces (e.g., hard spheres), the cross section corresponds to their geometric size.
        %
        For long range forces however, the cross section is larger than the physical dimensions of the particles due to action--at--a--distance effects.
    \end{definition}
%

    \begin{definition}
        The \emph{differential cross section} (\(\frac{d\sigma}{d\Omega}\)) provides additional granularity by describing how the probability of scattering depends on specific final state variables, such as scattering angle (\(\theta\)) or energy transfer.
        %
        It is defined as
        \begin{equation}
            \frac{\dd\sigma}{\dd\Omega} = \frac{\text{Number of events scattered into } \dd\Omega}{\text{Incident flux} \times \text{Target density}}.
        \end{equation}
    \end{definition}

    The total cross section can be recovered by integrating over solid angle:
    \begin{equation}
    \sigma = \int_{4\pi} \frac{\dd\sigma}{\dd\Omega} \, \dd\Omega.
    \end{equation}

    While the classical picture above is intuitive, scattering at HEP experiments is governed by quantum field theory (QFT).
    %
    In this framework the probability for a process is encoded in a Lorentz invariant matrix element \(\mathcal{M}\).
    
    For a \(2 \to n\) reaction with incoming \(4-\)momenta \(p_{1,2}\) and final state phase space \( \dd\Phi_n \), the fully differential cross section is

    \begin{equation}
      \dd\sigma =
      \frac{(2\pi)^4 \, \delta^{(4)}\!\bigl(p_1 + p_2 - \sum_{i=1}^n p_i\bigr)}
           {4\,\sqrt{(p_1\!\cdot\!p_2)^2 - m_1^2 m_2^2}}
      \; |\mathcal{M}|^2 \;
      \dd\Phi_n,
      \label{eq:qft_xsec}
    \end{equation}
    where the denominator is the flux factor and \(\dd\Phi_n = \prod_{i=1}^{n} \tfrac{\dd^3 p_i}{(2\pi)^3 2E_i}\) is the Lorentz invariant phase space element.\footnote{Standard derivations can be found in \cite{peskin_introduction_1995,Navas2024ReviewPhysics, Hollik2014Quantum978-1-107-03473-0, QuantumAssessment}.
    }
    %
    \cref{eq:qft_xsec} reduces to the classical area when \(|\mathcal{M}|^2\) is replaced by a contact interaction and the final state integral collapses to a single kinematic configuration.
    %
    Integrating \cref{eq:qft_xsec} over final state kinematics yields the total cross section, \(\sigma = \int \dd\sigma\).
    
    At tree level, \(|\mathcal{M}|^2\) is computed from Feynman rules derived from the Lagrangian, while higher order corrections incorporate loops, parton showers, and non-perturbative effects such as hadronisation.
    %
    For practical experimental predictions predictions one folds \(|\mathcal{M}|^2\) with parton distribution functions (PDFs) and convolves the result with detector response---precisely the forward process that the unfolding methods developed in this thesis seek to invert.

Differential cross sections have a long history of providing valuable insights for probing fundamental properties of particles and interactions.
%
Their use dates all the way back to Rutherford's scattering experiments that revealed the existence of atomic nuclei by analysing angular distributions of scattered alpha particles \cite{F.R.S.1911LXXIX.Atom}.

    \subsection{Experimental measurement.}
    \label{subsec:exp_measurement}
        At modern colliders\footnote{
            This section focusses on collider experiments, but similar analyses can be applied to non-collider HEP experiments as well, such as fixed target experiments.
            %
            \cite{leo_techniques_1994} and chapters \numrange{31}{38} of \cite{particle_data_group_review_2022} provide a comprehensive and detailed exposition of experimental measurement in HEP.
            %
            \cite{Brodsky2013PhysicsBeams, AveryCrossRates} focus specifically on fixed target techniques and phenomenology.
            %
            \cite{MuheimNuclearLaws} compares collider and fixed target formalisms, reviews luminosity analogues, and details the role of detector simulations and unfolding in a fixed target context
        } the two beams themselves act as both ``projectile'' and ``target.'' 
        %
        The basic experimental quantity is the instantaneous luminosity.
        \begin{definition}
            The \emph{luminosity} \(\mathcal{L}(t)\) is defined such that the interaction rate for a process with cross section \(\sigma\) is \(\dd N/\dd t = \mathcal{L}(t)\,\sigma\).
        \end{definition}

        Time integrating over a data taking period \([t_0, t_f]\) yields the integrated luminosity.
        \begin{definition}
            The \emph{integrated luminosity} \(\mathcal{L}_{\text{int}}\) is defined as
            \begin{equation}
                \mathcal{L}_{\text{int}}
                = \int_{t_0}^{t_f} \mathcal{L}(t)\,\dd t,
                \qquad
                \sigma
                = \frac{N_{\text{obs}} - N_{\text{bkg}}}
                       {\mathcal{L}_{\text{int}}\,\epsilon\,A}.
                \label{eq:crossec_collider}
              \end{equation}
              Here \(N_{\text{obs}}\) is the number of selected events, \(N_{\text{bkg}}\) an estimate of background contaminations, \(\epsilon\) the detector and selection efficiency and \(A\) the geometric--plus--kinematic acceptance of the analysis.
        \end{definition}


        For binned measurements one bins events in an observable \(X\)\footnote{For example, transverse momentum \(p_T\) or rapidity \(y\)} and divides by the bin width.
        \begin{equation}
          \frac{\dd\sigma}{\dd X}\Big|_{X_i}
          = \frac{1}{\mathcal{L}_{\text{int}}\,\Delta X_i}
            \frac{N_i^{\text{obs}} - N_i^{\text{bkg}}}
                 {\epsilon_i\,A_i},
          \label{eq:diff_crossec_collider}
        \end{equation}
        with the index \(i\) denoting the \(i^{\text{th}}\) bin.

        Luminosity determination is itself a precision measurement, usually performed with dedicated luminometers that exploit van der Meer scans or pileup counting techniques.
        %
        The efficiency--acceptance term \(\epsilon\,A\) is obtained from full detector simulations and corrected in data using control samples and ``tag--and--probe'' methods.

        \cref{eq:crossec_collider,eq:diff_crossec_collider} thus link the theoretically calculated parton level cross sections (\textit{vide}~\cref{eq:qft_xsec}) to the raw observables recorded by the a detector, completing the chain from theory to experiment.

    \subsection{Applications in particle physics.}
        As mentioned above, cross section measurements serve as the fundamental currency of particle physics, translating abstract theoretical predictions into measurable experimental quantities.
        %
        However, their applications extend far beyond simple theory validation into the operational heart of how experiments function, analyse data, and cross--validate results.
        \subsubsection{Theory validation.}
            As discussed above, comparing measured cross sections with predictions from quantum field theory validates and tests theoretical models like quantum chromodynamics and electroweak theory, by encapsulating interaction probabilities in a measurable form.
            %
            Deviations from expected cross sections may indicate new phenomena, such as supersymmetric particles or dark matter candidates.
            
            Differential cross sections also provide constraints on effective field theories and parton distribution functions (PDFs), essential for understanding the internal structure of hadrons.
            %
            Unfolded cross section measurements allow comparisons with theoretical models years after data collection, even if detector simulations are no longer available, further enhancing their utility, and future proofing the data.
            %
            Their determination requires careful design and analysis techniques to account for systematic uncertainties introduced by detector effects.

        \subsubsection{Monte Carlo tuning.}
            \begin{definition}
                \emph{Monte Carlo tuning} is the iterative process of adjusting simulation parameters to match measured cross sections, ensuring that detector simulations accurately reproduce real experimental data.
            \end{definition}
            %
            Cross section measurements are extensively used in Monte Carlo (MC) tuning.
            
            Every particle physics analysis relies on sophisticated simulations that model everything from the initial parton interactions through hadronisation to the detector response.
            %
            These simulations contain dozens of phenomenological parameters, such as the strong coupling constant at various scales and non-perturbative fragmentation functions.
            \begin{definition}
                \emph{Parton distribution functions} (PDFs) are probability distributions that describe the probability of finding a parton (quark or gluon) in a hadron at a given momentum fraction \(x\) and scale \(Q^2\).
                %
                PDFs are determined from global fits to a wide range of hard scattering processes, including deep inelastic scattering, Drell--Yan production, and jet production.
            \end{definition}
            %
            Measured cross sections provide the ground truth that anchors these simulations to reality.

            Consider the following example:
            %
            When CMS measures the inclusive jet cross section at a new centre--of--mass energy, that measurement immediately becomes a crucial input for tuning generators like \textsc{Pythia} or \textsc{Herwig}.
            %
            The differential distributions---whether in transverse momentum, rapidity, or invariant mass---reveal where the models succeed and where they fail.
            %
            A discrepancy in the high\(-p_T\) tail might indicate a need to adjust the modelling of initialstate radiation; unexpected structure in angular distributions could point to missing higher order QCD effects.
        \subsubsection{Consistency checks.}
            Cross--validations between different experimental approaches, detector configurations, or analysis methods ensure measurement reliability and identify systematic biases.
            %
            Cross sections serve as essential tools for consistency checks across multiple dimensions of experimental physics.
            %
            Within a single experiment, measuring the same process through different decay channels provides a powerful systematic cross check.
            %
            For instance, measuring the W boson production cross section through both electronic and muonic decays tests the understanding of lepton universality while simultaneously validating detector calibrations.
            %
            Any significant deviation signals either new physics, errors in the phenomenological method used, or unaccounted systematic effects.
            
            Between experiments, cross section measurements enable crucial cross--experiment validation.
            %
            When CMS and ATLAS measure the same process with independent detectors and analysis chains, agreement within uncertainties validates both measurements.
            %
            Disagreement, conversely, can reveal subtle systematic effects or push calculations to higher precision.

            These applications cascade through every level of experimental operations.
            %
        \subsubsection{Luminosity determination.}
            Luminosity determination becomes possible through processes with well--known theoretical cross sections.
            %
            Van der Meer scans calibrate the absolute luminosity scale, but elastic scattering and other standard candle processes provide continuous monitoring.
            %
            The uncertainty on integrated luminosity, typically \numrange{2}{3}\%, directly impacts every cross section measurement, creating a web of interdependencies.
            
        \subsubsection{Background estimation.}
            Background estimation in searches for new physics relies on measured cross sections of Standard Model processes.
            %
            When searching for supersymmetric particles, the irreducible backgrounds from \(W\, +\) jets or \(t\bar t\) production must be understood at the percent level.
            %
            Control regions enriched in backgrounds, combined with precise cross section measurements, enable data driven background estimates that would be impossible from simulation alone.
        \subsubsection{Parton luminosity.}
            \begin{definition}
                \emph{Parton luminosity} is the effective luminosity for specific parton--parton interactions, calculated by convolving the total luminosity with parton distribution functions.
            \end{definition}
            %
            A particularly elegant application of cross sections emerges in parton luminosity calculations.
            %
            Since protons are composite objects, the effective luminosity for producing heavy particles depends on the convolution of PDFs with the partonic cross section.
            %
            Measurements of Drell--Yan production at different invariant masses directly probe the quark and antiquark distributions, while inclusive jet production constrains the gluon PDF.
            %
            This creates a self--consistent feedback loop, where better PDFs enable more precise predictions, which enable more sensitive measurements, which further constrain the PDFs.
            
        \subsubsection{Detector performance validation.}
             Detector performance validation represents another major application of cross section measurements.
             %
             Measured cross sections for well understood processes serve as standard candles for monitoring detector stability over time.
             %
             For example, slow drift in the measured \(Z\to\mu\mu\) cross section might indicate degrading muon chamber performance long before it would be noticed in individual event displays.
             %
             These measurements become part of the experiment's data quality monitoring, flagging problems in real time.
            \subsubsection{Systematic uncertainty evaluation.}
                The role of cross section measurements in systematic uncertainty evaluation cannot be overstated.
                %
                Every measurement must account for theoretical uncertainties in signal and background processes.
                %
                By measuring auxiliary cross sections, for instance, \(Z\, +\) jets production when studying \(W\, +\) jets, experiments can constrain these uncertainties using data rather than relying solely on theoretical estimates.
                %
                This \textit{in situ} constraint often reduces systematic uncertainties by factors of two or more.
            \subsubsection{Future planning.}
                Finally, cross sections enable physics program planning for future experiments.
                %
                The measured production rates at current energies, extrapolated using theoretical calculations, determine required luminosities and detector capabilities for next--generation experiments.
                %
                For exmaple, the surprisingly large Higgs production cross section at the LHC, for instance, has already influenced design considerations for future electron--positron Higgs factories.
    \vskip{1em}            
    In sum, cross sections certainly do function as the Rosetta Stone of particle physics, translating between the languages of theory, simulation, and experimental measurement while maintaining coherence across all three domains.
    %
    However, they serve not merely as endpoints of analyses, but as the connective tissue that binds together theory, simulation, and experiment into a coherent whole.
    %
    They simultaneously test theoretical understanding, calibrate experimental tools, and point the way toward new discoveries.
    %
    This multiplicative utility explains why cross section measurements, even of well studied processes, remain at the heart of every particle physics experiment.

\section{Detector response in precision measurements.}
    The direct comparison between theoretical predictions and experimental measurements is complicated by detector effects.
    %
    HEP detectors are technological marvels that capture the trajectories of charged particles, energy deposits in calorimeters, and timing and pattern--recognition information from tracking and particle--identification systems, but they introduce distortions that must be carefully accounted for to extract the true physical distributions of interest.
    %
    Particle physics detectors represent some of humanity's most sophisticated sensing apparat\=us—--capturing particle trajectories with silicon sensors operating at liquid helium temperatures, measuring energy deposits in dense calorimeter crystals, and reconstructing vertices with sub--millimetre precision.
    %
    Yet these technological marvels inevitably introduce systematic distortions that transform the pristine theoretical predictions into the messy reality of experimental data.
    
    Consider the information degradation that occurs in every measurement. 
    %
    Finite resolution creates fundamental blurring, much like how a camera lens distorts an image.
    %
    The detector's discrete sensing elements can only measure particle energies, momenta, and positions to finite precision, creating an inherent convolution between the true physics distribution and the instrument response function.
    %
    Geometric acceptance imposes hard boundaries on observable phase space.
    %
    Particles scattered into the forward beam pipe or extreme backward angles simply vanish from the recorded dataset, creating holes in the measurement that no amount of statistics can fill.
    
    Detection efficiency varies across the detector's active volume, introducing a complex weighting function that depends on particle type, energy, and trajectory.
    %
    A high--energy muon might traverse the entire detector with near--perfect efficiency, while a low--energy hadron could be absorbed in the first layers of material.
    %
    Particle misidentification compounds these challenges through cross--contamination between categories, such as when hadronic shower fluctuations cause a pion to masquerade as a kaon~\cite{belle_collaboration_precision_2013}.

    In this way every detector measurement embeds two irreducible probability relationships.
    \begin{description}
        \item [Detection incompleteness] \(\P(\text{measured}\mid\text{true}) < 1\) True events that fail to be recorded
        \item [Measurement impurity] \(\P(\text{true}\mid\text{neasured}) < 1\) Recorded events that represent contamination
    \end{description}
    This demonstrates why detector corrections are fundamentally different from simple calibrations.
    %
    Unlike adjusting a scale that consistently reads, say, \(5\%\) high, detector response involves dual information loss that operates asymmetrically.
    
    The first inequality captures the selection bias where certain true configurations have zero probability of detection, creating null spaces in the measurement.
    %
    The second inequality captures the contamination bias where every reconstruction category contains some fraction of misclassified events.
    
    Background contamination represents a third problem---every measurement category contains some admixture of misclassified events.
    %
    When hadrons interact in electromagnetic calorimeters, they can mimic electron signatures.
    %
    When cosmic ray muons traverse the detector during a collision, they contribute to the muon count despite having no connection to the physics of interest.
    %
    This contamination creates what in signal processing is called the false positive rate.
    
    The mathematical relationship between true particle level distributions and observed detector level measurements follows the convolution integral
    \[
        \label{eq:forward-folding}
        p(x) = \int r(x\mid z)\;p(z)\; \dd z
    \]
    Where \(p(x)\) is the detector level density, \(p(z)\) is the particle level density and \(r(x\mid z)\) serves as the response kernel, the conditional probability density that maps each possible true configuration \(z\) to the distribution of possible detector measurements \(x\).
    %
    This kernel encapsulates the entire cascade of finite resolution information degradation.
    
    The response kernel is analogous to the optical transfer function in image processing.
    %
    It describes how the ``lens'' of the detector blurs and distorts the perfect theoretical ``image''.
    %
    The response function inherently embeds both probabilistic asymmetries.
    %
    Regions where \(\int r(x\mid z)\;\dd x < 1\) reveal acceptance holes, i.e. true configurations that produce no detector signal whatsoever.
    %
    Conversely, the convolution structure itself ensures that multiple truth distributions can yield identical detector observations, creating the degeneracy problem that makes direct inversion impossible.
    
    A central challenge then, for particle physics, is inverting this response kernel to recover \(p(z)\) from observed data \(p(x)\).
    %
    This inversion is mathematically ill--posed precisely because of the information loss.
    %
    Standard matrix inversion fails catastrophically, amplifying statistical noise into wild oscillations that bear no resemblance to the underlying physics.
    
    The resolution requires sophisticated regularisation methods that impose additional constraints such as smoothness assumptions, positivity requirements, and prior knowledge about the expected signal shape.
    %
    These constraints transform the ill--posed inverse problem into a well defined statistical inference challenge, though at the cost of introducing systematic uncertainties that must themselves be carefully validated.
    
    The detector response problem exemplifies a universal pattern in experimental science: the tension between instrumental precision and information preservation.
    %
    From astronomical imaging through medical diagnosis to HEP measurements, the fundamental trade off between sensitivity and purity governs all attempts to extract signal from noise.

\section{Challenges at modern experiments.}
    Several challenges at the modern HEP experiments make cross section measurements particularly demanding.

    \begin{description}
        \item [High dimensional phase spaces] Modern measurements often involve multiple correlated observables, creating high dimensional distributions that are difficult to analyse with traditional methods.
        \item [Limited statistics in extreme regions] Rare processes or the tails of distributions often contain valuable physics information but suffer from limited statistics.
        \item [Complex detector effects] Detectors have non-trivial response functions that can vary significantly across phase space, and are only known implicitly through precision simulations. Their explicit functional form is unknown.
        \item [Theoretical uncertainties] Precision measurements are increasingly limited by theoretical uncertainties in both signal and background modelling.
        \item [Computational constraints] Detailed simulation of detector response requires substantial computing resources, limiting the statistical precision of response modelling.
    \end{description}

    These challenges make the unfolding problem increasingly difficult, particularly as measurements probe more complex final states and differential distributions.
    %
    For example, measurements of jet substructure, which probe the detailed radiation pattern within collimated sprays of particles, involve observables with complex correlations and detector effects that vary based on jet energy, rapidity, and substructure properties themselves~\cite{Larkoski2020JetLearning, kogler_jet_2019, mozer_jet_2017}.
    
    The need for unfolding arises from the fundamental requirement to present results in a detector independent form that can be directly compared with theory predictions or results from different experiments.
    %
    Without this correction, theoretical interpretations would need to incorporate experiment specific detector simulations, significantly complicating scientific exchange and theoretical analysis, and inter--experiment comparisons would simply not be possible.

\section{Thesis scope and physics impact.}

This dissertation focuses on developing, analysing, and applying novel machine learning methods for cross section measurements in particle physics, with particular emphasis on unbinned approaches that overcome limitations of traditional techniques. The work spans the spectrum from improving binned methods with neural posterior estimation to completely unbinned approaches for both full distributions and statistical moments.

The primary contributions of this thesis include:

\begin{enumerate}
\item Development of \textsc{Neural Posterior Unfolding} (NPU), enhancing binned approaches through normalising flows and amortised inference.
\item Introduction of \textsc{Moment Unfolding}, directly deconvolving distribution moments without binning.
\item Creation of \textsc{Reweighting Adversarial Networks} (RAN), a general framework for unbinned spectrum unfolding.
\item Analysis of event correlations in unfolded data and their impact on uncertainty estimation.
\item Investigation of symmetry discovery with \textsc{SymmetryGAN} and its connections to measurement constraints.
\end{enumerate}

These methodological advances address fundamental challenges in experimental particle physics, potentially enhancing the precision and scope of measurements at present and future HEP experiments.
%
These methods can have a wide range of applications in particle physics, including:

\begin{itemize}
\item Improved precision in jet substructure measurements, enabling better discrimination between different theoretical models of QCD radiation.
\item Enhanced sensitivity to effective field theory parameters by directly deconvolving distribution moments.
\item More robust uncertainty quantification in high dimensional measurements.
\item Computational efficiency gains allowing for more detailed systematic studies.
\item A rigorous statistical framework for incorporating detector response uncertainties in the unfolding process.
\end{itemize}

By bridging sophisticated machine learning techniques with the specific requirements of particle physics measurements, this work aims to advance the ability to extract fundamental physical insights from complex experimental data.
%
The methods developed here have applications beyond particle physics, potentially benefiting any field where deconvolution of instrumental effects is necessary for scientific inference.